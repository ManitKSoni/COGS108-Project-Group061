{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissions\n",
    "[x]No - keep private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Manit Soni\n",
    "- Young Jun Kim\n",
    "- Esau Estrada\n",
    "- Cooper Beaman\n",
    "- Arren De Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members IDs\n",
    "\n",
    "- A15576567\n",
    "- A16062404\n",
    "- A16056905\n",
    "- A13589935\n",
    "- A15697684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project explores how different sources of news media portrayed the candidates of the 2016 presidential election; namely Hillary Clinton and Donald Trump.  We look at articles from three news outlets:  CNN (moderate leaning), New York Times (left leaning), and Breitbart (right leaning). Using web scraping to gather our data followed by sentiment analysis to quantify candidate portrayals, we present a preliminary analysis of potential trends in sentiment between outlets and candidtates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do one to two word phrases tell us about how different news outlets covered Donald Trump and Hillary Clinton in the 2016 general elections? Did news outlets predominately describe each candidate positively or negatively? Were certain phrases more commonly used by news outlets when describing candidates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been much discussion over the words politicians use to convey their ideas. Their words are meant to convince the public and thus are catered for that effect. Many analyses have been done on the keywords politicians use and their connotations to the subjects they discuss. Our objective is to apply this same methodology to the words news organizations themselves use to describe political figures, focusing on the presidential candidates of the 2016 election.\n",
    "\n",
    "We are interested in this questions as we often hear about fake news and media narratives and want to confirm the validity or falsity of this statement. The importance of this topic is in understanding the information we consume, especially as it is often taken for granted to be accurate.\n",
    "\n",
    "There has already been research done to look at the headlines used during President Trump's term in office. Towards Data Science concluded that 'trump' was the most used word by quite a landslide. CNN and MSNBC used many keyword associated with trump's scandals, whereas FOX used more generic words such as 'man' and 'woman'. The New Yorker analyzed the language used by the political condidates during the 2016 election. For example, the Democratic candidates tended to use more concrete language and the Republican candidates tended to use more descriptive language.\n",
    "\n",
    "\n",
    "References (include links):\n",
    "- [1] https://blog.gdeltproject.org/announcing-the-television-news-ngram-datasets-tv-ngram/\n",
    "- [2] https://www.newyorker.com/magazine/2016/04/11/examining-the-vocabulary-of-the-presidential-race\n",
    "- [3] https://towardsdatascience.com/how-does-news-coverage-differ-between-media-outlets-20aa7be1c96a\n",
    "- [4] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6549470/\n",
    "- [5] https://www.newyorker.com/magazine/2018/10/01/how-russia-helped-to-swing-the-election-for-trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "Not all news outlets use words of equally negative sentiment in their articles, however all news outlet articles and headlines analyzed by Michael Tauberg at Towards Data Science exhibited a negative sentiment bias on average [3].  Additionally, during presidential elections in general, candidates typically search for negative events from their opponent's past to reduce their chances of winning. Furthermore, most humans tend to prefer gossip compared to other forms of conversation [4]. Hence, news outlets are incentivized to report negative stories, true or not from candidate's campaigns instead of more positive stories to increase viewership and revenue more reliably. Thus, we predict both Hillary Clinton and Donald Trump should have been portrayed negatively by most news outlets. However, given the abnormally high frequency of \"Trump\" appearing in news articles combined with his infamous personality [3,5], we predict news outlet's negative sentiment descriptions of Trump should predominate relative to those of Hillary. \n",
    "\n",
    "### Hypothesis\n",
    "__We predict all news networks we analyze will cover both 2016 presidential candidates with predominantly negative sentiment, however we expect to see greater negative sentiment descriptions of Donald Trump compared to Hillary Clinton.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Name: News Sentiments Towards Presidential Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Source:\n",
    "We are collecting data, through web scraping, from online news articles.  The three news sources we will be scraping are:\n",
    "* CNN\n",
    "* Breitbart\n",
    "* New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the three news sources, we are reading in: \n",
    "* 9272 articles from CNN\n",
    "* 150  articles from Breitbart\n",
    "* 1294 articles from New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up our data into a dataframe, as described in the next sections, our data will be organized in columns as follows:\n",
    "\n",
    "| Column Name | Description                                            |\n",
    "|-------------|--------------------------------------------------------|\n",
    "| **News_Source** | The host of the article. Either CNN, Breitbart, or NYT |\n",
    "| **Candidate**   | The name of the candidate. Either Trump or Hillary     |\n",
    "| **neg**         | The negative sentiment score. Between 0 and 1          |\n",
    "| **neu**         | The neutral sentiment score. Between 0 and 1           |\n",
    "| **pos**         | The positive sentiment score. Between 0 and 1          |\n",
    "| **compound**    | The combined sentiment score. Between 0 and 1          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a condensed representation of our dataset at the end of the **Data Cleaning** section.  After applying sentiment analysis onto each sentence, our cleaned dataset has 137599 observations. However, the analysis would have unfair comparisons since some news organizations had more articles and references to Trump/Hillary. We reduced the number of observations from CNN and NYT to match Breitbart through random sampling. Thus, the dataset was condensed to **6479** observations, equally split into three parts between the news sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Firstly, we went through each of the news sources and scraped a list of all US presidential election articles in the year 2016.  The scripts for these can be scene in our Scripts folder, called [News Source]ArticleFinder.cpp, and they are pretty straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data sources, it is now time to: \n",
    "* webscrape the contents of those articles\n",
    "* perform sentiment analysis on each sentence \n",
    "* connect each sentence to a candidate \n",
    "* write all that data to a csv. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done in the LinkToText.ipynb file in the Scripts folder. This webscraping takes hours and is thus NOT recommended to be run. The data has alrady been collected and saved in the Dataset.csv file in the Scripts folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our data into a nice CSV, we will now parse the data into a pandas dataframe and clean it.  For cleaning, we will be removing data that contains a 0 value for neg, pos, neu, and compound since this means that our sentiment analyzer was not able to work on that particular sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our data set\n",
    "df = pd.read_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentiment into its own columns\n",
    "df[['neg','neu', 'pos', 'compound']] = df.Sentiment.str.split(\",\",expand=True)\n",
    "\n",
    "# Drop sentinment, we dont need it anymore\n",
    "df = df.drop(['Sentiment'], axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the extraneous parts of the strings\n",
    "def just_digits(label):\n",
    "    numbers = re.compile('\\d+(?:\\.\\d+)?')\n",
    "    return numbers.findall(label)[0]\n",
    "\n",
    "# Convert each of those columns to float type\n",
    "df['neg'] = df['neg'].apply(just_digits).astype(float);\n",
    "df['neu'] = df['neu'].apply(just_digits).astype(float);\n",
    "df['pos'] = df['pos'].apply(just_digits).astype(float);\n",
    "df['compound'] = df['compound'].apply(just_digits).astype(float);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News_Source</th>\n",
       "      <th>Candidate</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Trump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Trump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Trump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Trump</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Trump</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139593</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139594</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.0387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139595</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.0387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139596</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.7430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139597</th>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Hillary</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.7430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137599 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       News_Source Candidate    neg    neu    pos  compound\n",
       "2              CNN     Trump  0.000  1.000  0.000    0.0000\n",
       "3              CNN     Trump  0.000  0.813  0.187    0.3182\n",
       "4              CNN     Trump  0.000  0.813  0.187    0.3182\n",
       "5              CNN     Trump  0.081  0.827  0.091    0.0772\n",
       "6              CNN     Trump  0.066  0.838  0.095    0.2500\n",
       "...            ...       ...    ...    ...    ...       ...\n",
       "139593   Breitbart   Hillary  0.065  0.873  0.062    0.0240\n",
       "139594   Breitbart   Hillary  0.000  0.946  0.054    0.0387\n",
       "139595   Breitbart   Hillary  0.000  0.946  0.054    0.0387\n",
       "139596   Breitbart   Hillary  0.042  0.725  0.233    0.7430\n",
       "139597   Breitbart   Hillary  0.042  0.725  0.233    0.7430\n",
       "\n",
       "[137599 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where ALL values are 0\n",
    "df = df[(df[['neg', 'neu', 'pos', 'compound']] != 0).any(axis=1)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this state, we have a dataframe that contains columns for News_Source, Candidate, neg (negative sentiment score), neu (neutral sentiment score), pos (positive sentiment score), and compound (compound sentiment score). Using this, we will be able to manipulate and analyse the data further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data did not include any null or invalid data. Essentially, the data would have been invalid if there was a 0 value for \"pos\", \"neg\", and \"neu\". However, this is not possible through the use of the sentiment analyser, which would give a neutral value of 1 if there was no trace of positive or negative sentiment. We did have to clean up the data by breaking the sentiment output into \"pos\", \"neg\", \"neu\", and \"compound\". Thus we could graph these float values instead of the string sentiment output. apart into However, most of these sentences are just neutral, meaning they were not analyzed as having too much positive or negative sentiment.  The reason for this is that since we are reading in all sentences, many sentences are full of filler words, conjugations, etc that just convey information. So in the following section, we will be using different thresholds of neutral sentences (i.e. < 0.8 neu or < 0.5 neu) in order to just look at the more *emotionally* charged sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any privacy concerns with the data used as it will be taken from news articles and coverage that is already available to the public. There may be potential biases in the dataset if it doesn't use diverse sources or overly relies on select sources. This is because of the potential political biases of news outlets. We control for this by having variables for the specific news organization. Thus, we will have multiple perspectives on the coverage instead of a combined media perspective. There may also be an unintended use of the data to promote narratives of why the coverage is directed the way it is. Our research is specifically meant to look for correlations of positive/negative coverage of candidates and does not prove any bias towards/against candidates.\n",
    " \n",
    "One thing we have to worry about in terms of ethics is that news outlets may have certain ideas they would want to promote. For example, a news source might have policies on what their hosts can talk about and how they should present the information to the public. If we do not account for this fact, it can raise ethical issues because it can skew our data with the biases of particular news outlet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manit and Aaron wrote the main web scrapping algorithms\n",
    "- Young Jun, Esau, and Cooper analyzed the data, created questions, and made adjustments to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
